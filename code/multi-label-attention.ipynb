{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delivery version 10 - Pseudo labeling - LSTM64-version\n",
    "\n",
    "### Changed to batch 768 for faster\n",
    "\n",
    "Model for creating pseudolabels: 2x5epoch 3x64LSTM\n",
    "\n",
    "Main model: 3x5poch  2x128Bi-LSTM\n",
    "\n",
    "public test set size: 56k, private teste set size: 376k.\n",
    "\n",
    "\n",
    "This version has 3 lanes of embedding and also 2x and 3xLSTM64, in addition to old 2xLSTM128\n",
    "Has focal loss.\n",
    "\n",
    "Text preprocessing is done with lower() as it tested to give better accuracy than leaving capital letetrs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quora Insincere Questions Classification\n",
    "\n",
    "https://www.kaggle.com/c/quora-insincere-questions-classification"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Recorded running times\n",
    "Delivery5  2x64LSTM\n",
    "Dataframes loaded. Time passed: 0.10 min \n",
    "Text cleaned. Time passed: 1.39 min \n",
    "Tokenizing done. Time passed: 2.49 min \n",
    "Embeddings ready. Time passed: 9.90 min \n",
    "Finished main training at: 69.04 min \n",
    "Training done. Time passed: 69.04 min Duration of training: 44.42 min \n",
    "End of code reached. Total time: 69.21 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_LOCAL = False # Set this false to run in Kaggle - removes printouts, local dev set prediction etc.\n",
    "TRAIN_PERCENT = 0.9 #developing 0.9\n",
    "\n",
    "batch_size = 768 # how many samples to process at once  - 768 around 10% faster in Kaggles K80 than 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = 'glove'\n",
    "n_epochs = 4 # how many times to iterate over all samples\n",
    "max_features = 88000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 50 # max number of words in a question to use\n",
    "embed_size = 300 # how big is each word vector\n",
    "SEED = 1006\n",
    "SEED = SEED+2\n",
    "LOGFILE = 'logfile.txt'\n",
    "folds = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/max/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import os\n",
    "from sklearn.metrics import f1_score\n",
    "import re\n",
    "import torch\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, CuDNNLSTM\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm_notebook, tnrange\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas(desc='Progress')\n",
    "\n",
    "import string\n",
    "\n",
    "# Display whole text of dataframe field and don't cut it\n",
    "pd.set_option('display.max_colwidth', -1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import multiply, concatenate \n",
    "from keras import backend as K\n",
    "# import keras.activations\n",
    "from keras.layers import Concatenate\n",
    "\n",
    "# starting capital is a Layer, starting small is a function. Use layers.\n",
    "from keras.layers import Multiply\n",
    "from keras.layers import MaxPooling1D, Conv1D, SpatialDropout1D\n",
    "from keras.layers.pooling import AveragePooling1D\n",
    "from keras.layers.normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDINGS_PATH = '../input/embeddings/'\n",
    "EMBEDDING_FILE_GLOVE = f'{EMBEDDINGS_PATH}/glove.840B.300d/glove.840B.300d.txt'\n",
    "EMBEDDING_FILE_FAST = f'{EMBEDDINGS_PATH}/wiki-news-300d-1M/wiki-news-300d-1M.vec'\n",
    "\n",
    "EMBEDDING_FILE_PARA = f'{EMBEDDINGS_PATH}/paragram_300_sl999/paragram_300_sl999.txt'\n",
    "EMBEDDING_FILE_WORD2VEC = f'{EMBEDDINGS_PATH}/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPEATABILITY\n",
    "def seed_everything(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything()\n",
    "# kernel https://www.kaggle.com/hengzheng/pytorch-starter\n",
    "\n",
    "# https://www.kaggle.com/ziliwang/baseline-pytorch-bilstm\n",
    "def bestThresshold(train_y,train_preds):\n",
    "    tmp = [0,0,0] # idx, cur, max\n",
    "    delta = 0\n",
    "    for tmp[0] in tqdm(np.arange(0.1, 0.501, 0.01)):\n",
    "        tmp[1] = f1_score(train_y, np.array(train_preds)>tmp[0])\n",
    "        if tmp[1] > tmp[2]:\n",
    "            delta = tmp[0]\n",
    "            tmp[2] = tmp[1]\n",
    "    print('best threshold is {:.4f} with F1 score: {:.4f}'.format(delta, tmp[2]))\n",
    "    return delta\n",
    "\n",
    "def train_validate_test_split(df, df_y, train_percent=.6, validate_percent=.2, random_state=10):\n",
    "    np.random.seed(random_state)\n",
    "    perm = np.random.permutation(len(df))\n",
    "    m = len(df)\n",
    "    train_end = int(train_percent * m)\n",
    "    validate_end = int(validate_percent * m) + train_end\n",
    "    train = df[perm[:train_end]]\n",
    "    validate = df[perm[train_end:validate_end]]\n",
    "    test = df[perm[validate_end:]]\n",
    "    \n",
    "    train_y = df_y[perm[:train_end]]\n",
    "    validate_y = df_y[perm[train_end:validate_end]]\n",
    "    test_y = df_y[perm[validate_end:]]\n",
    "\n",
    "    return train, validate, test, train_y, validate_y, test_y, perm\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    '''\n",
    "    metric from here \n",
    "    https://stackoverflow.com/questions/43547402/how-to-calculate-f1-macro-in-keras\n",
    "    '''\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "import tensorflow as tf\n",
    "# Keras version focal loss\n",
    "# usage loss=[focal_loss(alpha=.25, gamma=2)]\n",
    "def focal_loss(gamma=2., alpha=.25):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "\n",
    "        pt_1 = K.clip(pt_1, 1e-3, .999)\n",
    "        pt_0 = K.clip(pt_0, 1e-3, .999)\n",
    "\n",
    "        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0))\n",
    "    return focal_loss_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faster embedding loading - but cutting some words possibly\n",
    "# https://www.kaggle.com/syhens/speed-up-your-preprocessing\n",
    "#max_words=200000\n",
    "\n",
    "# def load_glove_slow(word_index, max_words=200000, embed_size=300)\n",
    "# add if i >= max_words: continue \n",
    "# instead of 'if i >= max_features: continue' ? or next to it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FUNCTIONS FROM https://www.kaggle.com/gmhost/gru-capsule\n",
    "\n",
    "def load_glove(word_index, max_words=1000000):\n",
    "    EMBEDDING_FILE = f'{EMBEDDINGS_PATH}/glove.840B.300d/glove.840B.300d.txt'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')[:300]\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n",
    "    \n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = -0.005838499,0.48782197\n",
    "    print(f'mean: {all_embs.mean()}, std: {all_embs.std()}')\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    # word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        #if i >= max_words: continue            \n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    return embedding_matrix \n",
    "    \n",
    "def load_fasttext(word_index):    \n",
    "    EMBEDDING_FILE = f'{EMBEDDINGS_PATH}/wiki-news-300d-1M/wiki-news-300d-1M.vec'\n",
    "    \n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100)\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    # word_index = tokenizer.word_index../input/embeddings/\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "def load_para(word_index):\n",
    "    EMBEDDING_FILE = f'{EMBEDDINGS_PATH}/paragram_300_sl999/paragram_300_sl999.txt'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, encoding=\"utf8\", errors='ignore') if len(o)>100)\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = -0.0053247833,0.49346462\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    # word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    #embedding_matrix = np.random.normal(emb_mean, 0, (nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "    \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(texts):\n",
    "    sentences = texts.apply(lambda x: x.split()).values\n",
    "    vocab = {}\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab\n",
    "#vocab = build_vocab(df['question_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/qqgeogor/keras-lstm-attention-glove840b-lb-0-043\n",
    "from keras.engine.topology import Layer\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        \"\"\"\n",
    "        Keras Layer that implements an Attention mechanism for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Example:\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(Attention())\n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        #self.init = initializations.get('glorot_uniform')\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # eij = K.dot(x, self.W) TF backend doesn't support it\n",
    "\n",
    "        # features_dim = self.W.shape[0]\n",
    "        # step_dim = x._keras_shape[1]\n",
    "\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "    #print weigthted_input.shape\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        #return input_shape[0], input_shape[-1]\n",
    "        return input_shape[0],  self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OMP_NUM_THREADS'] = '4'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"../input/train.csv\")\n",
    "df_test = pd.read_csv(\"../input/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>338652</th>\n",
       "      <td>42591771fa3a278f03f9</td>\n",
       "      <td>Are you a giant animal smart monkey or human with look like giant monkey but its smart?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14898</th>\n",
       "      <td>02ed6292809db66abc6a</td>\n",
       "      <td>Do you think any person will create a better group than groups like the AKC?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         qid  \\\n",
       "338652  42591771fa3a278f03f9   \n",
       "14898   02ed6292809db66abc6a   \n",
       "\n",
       "                                                                                  question_text  \\\n",
       "338652  Are you a giant animal smart monkey or human with look like giant monkey but its smart?   \n",
       "14898   Do you think any person will create a better group than groups like the AKC?              \n",
       "\n",
       "        target  \n",
       "338652  1       \n",
       "14898   0       "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Randomize\n",
    "np.random.seed(SEED)\n",
    "trn_idx = np.random.permutation(len(df_train))\n",
    "df_train = df_train.iloc[trn_idx]\n",
    "\n",
    "df = pd.concat([df_train ,df_test],sort=True)\n",
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(LOGFILE, \"a\") as myfile:\n",
    "    myfile.write(f'Dataframes loaded. Time passed: {(time.time()-start)/60:.2f} min\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9f6655373a140e5842837594aa2eb56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Progress', max=1306122, style=ProgressStyle(description_width='initial')), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
    " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
    " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "\n",
    "def clean_text(x):\n",
    "    x = str(x)\n",
    "    for punct in puncts:\n",
    "        if punct in x: # comparison makes faster\n",
    "            x = x.replace(punct, f' {punct} ')\n",
    "    return x\n",
    "\n",
    "df_train[\"question_text\"] = df_train[\"question_text\"].progress_apply(lambda x: clean_text(x))\n",
    "df_test[\"question_text\"] = df_test[\"question_text\"].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90095010d2ac4564b49522e18320cad7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Progress', max=1306122, style=ProgressStyle(description_width='initial')), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def clean_numbers(x):\n",
    "    x = re.sub('[0-9]{5,}', '#####', x)\n",
    "    x = re.sub('[0-9]{4}', '####', x)\n",
    "    x = re.sub('[0-9]{3}', '###', x)\n",
    "    x = re.sub('[0-9]{2}', '##', x)\n",
    "    return x\n",
    "\n",
    "df_train[\"question_text\"] = df_train[\"question_text\"].progress_apply(lambda x: clean_numbers(x))\n",
    "df_test[\"question_text\"] = df_test[\"question_text\"].apply(lambda x: clean_numbers(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5075959b773745289115dba179d2cd0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Progress', max=1306122, style=ProgressStyle(description_width='initial')), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "specials = {'\\u200b': ' ', '…': ' ... ', '\\ufeff': '', 'करना': '', 'है': ''}\n",
    "\n",
    "def clean_special_chars(text):\n",
    "    \n",
    "    for s in specials:\n",
    "        text = text.replace(s, specials[s])    \n",
    "    return text\n",
    "\n",
    "df_train[\"question_text\"] = df_train[\"question_text\"].progress_apply(lambda x: clean_special_chars(x))\n",
    "df_test[\"question_text\"] = df_test[\"question_text\"].apply(lambda x: clean_special_chars(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"question_text\"] = df_train[\"question_text\"].apply(lambda x: x.lower())\n",
    "df_test[\"question_text\"] = df_test[\"question_text\"].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32adc56d8cad40debf603f623a6cede3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Progress', max=1306122, style=ProgressStyle(description_width='initial')), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# CONTRACTIONS\n",
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }\n",
    "mispell_dict = {'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', 'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'Ethereum', 'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization', 'demonetisation': 'demonetization'}\n",
    "additional = {'fortnite': 'game', 'pubg': 'game'}\n",
    "\n",
    "# COMBINE BOTH FOR SINGLE RUN\n",
    "new_dict = {**mispell_dict, **contraction_mapping, **additional}\n",
    "mispell_dict = new_dict\n",
    "\n",
    "def _get_mispell(mispell_dict):\n",
    "    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
    "    return mispell_dict, mispell_re\n",
    "\n",
    "mispellings, mispellings_re = _get_mispell(mispell_dict)\n",
    "\n",
    "def replace_typical_misspell(text):\n",
    "    def replace(match):\n",
    "        return mispellings[match.group(0)]\n",
    "    return mispellings_re.sub(replace, text)\n",
    "\n",
    "df_train[\"question_text\"] = df_train[\"question_text\"].progress_apply(lambda x: replace_typical_misspell(x))\n",
    "df_test[\"question_text\"] = df_test[\"question_text\"].apply(lambda x: replace_typical_misspell(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(LOGFILE, \"a\") as myfile:\n",
    "    myfile.write(f'Text cleaned. Time passed: {(time.time()-start)/60:.2f} min\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save preprocessed versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_LOCAL:\n",
    "    df_train.to_csv('train_preprocessed.csv')\n",
    "    df_test.to_csv('test_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_LOCAL:\n",
    "    df_train = pd.read_csv('train_preprocessed.csv')\n",
    "    df_test =  pd.read_csv('test_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab = build_vocab(df['question_text'])\n",
    "#len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# larger than one\n",
    "# count = sum(1 for i in vocab.values() if i >= 2)\n",
    "# print(count)\n",
    "# 192036"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(val):\n",
    "    res = []\n",
    "    if val == 1:        res.append('is 1')    \n",
    "    if val > 1:        res.append('> 1')\n",
    "    if val > 2:        res.append('> 2')\n",
    "    if val > 3:        res.append('> 3')\n",
    "    if val > 4:        res.append('> 4')\n",
    "    if val > 5:        res.append('> 5')\n",
    "    if val > 6:        res.append('> 6')\n",
    "    if val > 7:        res.append('> 7')\n",
    "    if val > 8:        res.append('> 8')        \n",
    "    return res\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counter(classification for val in vocab.values() for classification in classify(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(vocab['crystal'])\n",
    "#print(vocab['crystals'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_sentences_train = df_train['question_text']\n",
    "list_sentences_test = df_test['question_text']\n",
    "list_sentences_combined = list_sentences_train.append(list_sentences_test, ignore_index=True)\n",
    "tokenizer = Tokenizer(num_words=max_features, filters='\\t\\n')\n",
    "tokenizer.fit_on_texts(list(list_sentences_combined))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "test_x = pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "train_y = df_train['target'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_LOCAL:\n",
    "    np.save('train_x_preprocessed.npy', train_x)\n",
    "    np.save('train_y_preprocessed.npy', train_y)\n",
    "    np.save('test_x_preprocessed.npy', test_x)\n",
    "# save tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_LOCAL:\n",
    "    train_x = np.load('train_x_preprocessed.npy')\n",
    "    train_y = np.load('train_y_preprocessed.npy')\n",
    "    test_x =  np.load('test_x_preprocessed.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(LOGFILE, \"a\") as myfile:\n",
    "    myfile.write(f'Tokenizing done. Time passed: {(time.time()-start)/60:.2f} min\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing entries in the embedding are set using np.random.normal so we have to seed here too\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLOVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: -0.005838498938828707, std: 0.4878219664096832\n",
      "3.56 min\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seed_everything()\n",
    "#embedding_matrix = load_glove(word_index)\n",
    "now = time.time()\n",
    "embedding_matrix = load_glove(word_index, max_words=400000)\n",
    "print(f'{(time.time() -now)/60:.2f} min\\n')\n",
    "\n",
    "# max_words=200k\n",
    "# mean: -0.005838498938828707, std: 0.4878219664096832\n",
    "# 3.54 min\n",
    "# 3,77 min other run"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "seed_everything()\n",
    "#embedding_matrix = load_glove(word_index)\n",
    "now = time.time()\n",
    "embedding_matrix = load_glove(word_index, max_words=200000)\n",
    "print(f'{(time.time() -now)/60:.2f} min\\n')\n",
    "\n",
    "# max_words=200k\n",
    "# mean: -0.005838498938828707, std: 0.4878219664096832\n",
    "# 3.47 min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### difference is small between 200k and 400k!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FASTTEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.58 min\n",
      "\n"
     ]
    }
   ],
   "source": [
    "now = time.time()\n",
    "seed_everything()\n",
    "embedding_matrix_fast = load_fasttext(word_index)\n",
    "print(f'{(time.time() -now)/60:.2f} min\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paragram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.73 min\n",
      "\n"
     ]
    }
   ],
   "source": [
    "now = time.time()\n",
    "seed_everything()\n",
    "embedding_matrix_para = load_para(word_index)\n",
    "print(f'{(time.time() -now)/60:.2f} min\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Was alternative - getting mean of embeddings"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Was alternative - getting mean of embeddings\n",
    "glove_embeddings = load_glove(word_index)\n",
    "paragram_embeddings = load_para(word_index)\n",
    "fasttext_embeddings = load_fasttext(word_index)\n",
    "\n",
    "embedding_matrix = np.mean([glove_embeddings, paragram_embeddings, fasttext_embeddings], axis=0)\n",
    "\n",
    "#del glove_embeddings, paragram_embeddings, fasttext_embeddings\n",
    "#gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Was alternative - Concatenate embeddings"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "embed_size = 600 # how big is each word vector\n",
    "embedding_matrix = np.concatenate((glove_embeddings, paragram_embeddings), axis=1)\n",
    "print(np.shape(embedding_matrix))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import sys\n",
    "sys.getsizeof(glove_embeddings)/1000000\n",
    "# 212 MB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save embedding matrix + vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_LOCAL:\n",
    "    np.save('embedding_matrix.npy', embedding_matrix)\n",
    "    np.save('word_index.npy', word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load embedding matrix + vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_LOCAL:\n",
    "    embedding_matrix = np.load('embedding_matrix.npy')\n",
    "    word_index = np.load('word_index.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(LOGFILE, \"a\") as myfile:\n",
    "    myfile.write(f'Embeddings ready. Time passed: {(time.time()-start)/60:.2f} min\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic example models"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "seed_everything()\n",
    "inp = Input(shape=(maxlen,))\n",
    "x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "# try 0.2 dropout\n",
    "x = Bidirectional(LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dense(50, activation=\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(1, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# CuDNN  - model direclty, without function\n",
    "seed_everything()\n",
    "inp = Input(shape=(maxlen,))\n",
    "x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "# try 0.2 dropout\n",
    "x = Dropout(0.2)(x)\n",
    "x = Bidirectional(CuDNNLSTM(128, return_sequences=False))(x)\n",
    "#x = Bidirectional(LSTM(50, return_sequences=False, dropout=0.2, recurrent_dropout=0.2))(x)\n",
    "x = Dropout(0.2)(x)\n",
    "#x = Bidirectional(CuDNNLSTM(50, return_sequences=True))(x)\n",
    "#x = Dropout(0.1)(x)\n",
    "#x = GlobalMaxPool1D()(x)\n",
    "x = Dense(50, activation=\"relu\")(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(1, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ver 14\n",
    "# 2x64LSTM\n",
    "# 64 lstm     Double embedding lines\n",
    "# 2x6 epoch for example\n",
    "\n",
    "# 128 to 84 sec with LSTM 128 -> 64: about 1/3 of time away\n",
    "# 84 -> 174 sec if emb trainable!!! slow. also as default seemed overfit\n",
    "\n",
    "#2 rounds. Average Test-F1: 0.68723\n",
    "#STD: 0.00101, low to high: 0.00202\n",
    "#Time: 14.991060626506805 min\n",
    "\n",
    "\n",
    "def create_model2x64():\n",
    "    \n",
    "    embedding_matrix1 = embedding_matrix # Glove\n",
    "    embedding_matrix2 = embedding_matrix_fast    \n",
    "    #embedding_matrix3 = embedding_matrix_para\n",
    "    \n",
    "    seed_everything()\n",
    "    inp = Input(shape=(maxlen,))\n",
    "    \n",
    "    # EMBEDDING 1\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix1], trainable=False)(inp) # trainable=True\n",
    "    x = Dropout(0.1)(x)\n",
    "    \n",
    "    #LSTM\n",
    "    attention_input = CuDNNLSTM(64, return_sequences=True)(x)\n",
    "\n",
    "    #LINE1: maxpool \n",
    "    max_pool = Dropout(0.2)(attention_input)\n",
    "    max_pool = MaxPooling1D()(max_pool) # size 25 when concatenating\n",
    "    #LINE2: avgpool\n",
    "    avg_pool = Dropout(0.2)(attention_input)\n",
    "    avg_pool = AveragePooling1D()(avg_pool)\n",
    "    #LINE3: attention\n",
    "    print(attention_input.shape) # input 256,\n",
    "    # attn 64 works for both lstm 128 and 64?\n",
    "    attention_probs = Dense(units=64, activation='softmax', name='attention_probs')(attention_input)  \n",
    "    print(attention_probs.shape)\n",
    "    attention_mul = Multiply()([attention_input, attention_probs])\n",
    "    print(f'Attention_mul shape: {attention_mul.get_shape()}') # (?, 50, 256)\n",
    "    print(max_pool) # (?, 25, 256)\n",
    "    \n",
    "    \n",
    "    # EMBEDDING 2\n",
    "    x2 = Embedding(max_features, embed_size, weights=[embedding_matrix2], trainable=False)(inp) # trainable=True!\n",
    "    x2 = Dropout(0.1)(x2)\n",
    "    \n",
    "    #LSTM\n",
    "    attention_input2 = CuDNNLSTM(64, return_sequences=True)(x2)\n",
    "    #LINE1: maxpool \n",
    "    max_pool2 = Dropout(0.2)(attention_input2)\n",
    "    max_pool2 = MaxPooling1D()(max_pool2) # size 25 when concatenating\n",
    "    #LINE2: avgpool\n",
    "    avg_pool2 = Dropout(0.2)(attention_input2)\n",
    "    avg_pool2 = AveragePooling1D()(avg_pool2)\n",
    "    #LINE3: attention\n",
    "    attention_probs2 = Dense(units=64, activation='softmax', name='attention_probs2')(attention_input2)  \n",
    "    attention_mul2 = Multiply()([attention_input2, attention_probs2])\n",
    "\n",
    "  \n",
    "    # CONCAT\n",
    "    #conc = Concatenate(axis=1)([attention_mul, max_pool, avg_pool, conv1d])\n",
    "    conc = Concatenate(axis=1)([attention_mul, max_pool, avg_pool, attention_mul2, max_pool2, avg_pool2])\n",
    "    \n",
    "    print(f'Concat shape: {conc.get_shape()}')\n",
    "    \n",
    "    #conc = Attention(512, 50)(conc)\n",
    "    conc = Attention(200+0)(conc) # 50 from attention (2-way lstm?), 25 from maxpooling1D, 25 from avg_pool, 11 CNN maxpool\n",
    "    \n",
    "    # linear, relu, bnorm, dropout, out\n",
    "    #conc = Dense(50, activation=\"relu\")(conc)\n",
    "    conc = Dense(100, activation=\"relu\")(conc)  # no change in running time if 50 or 10\n",
    "       \n",
    "    #conc = BatchNormalization()(conc) #effec? cost on speed?\n",
    "    conc = Dropout(0.1)(conc)\n",
    "    x = Dense(1, activation=\"sigmoid\")(conc)\n",
    "    return Model(inputs=inp, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ver13 Return sequences False.      Double embedding  \n",
    "# 4 epoch?             \n",
    "\n",
    "# on 512 batchsize 5 epoch 0.6915\n",
    "\n",
    "\n",
    "def create_model2x128():\n",
    "    embedding_matrix1 = embedding_matrix\n",
    "    embedding_matrix2 = embedding_matrix_fast\n",
    "    #embedding_matrix3 = embedding_matrix_para    \n",
    "    \n",
    "    seed_everything()\n",
    "    inp = Input(shape=(maxlen,))\n",
    "    # x = Embedding(max_features, embed_size, weights=[embedding_matrix1], trainable=True)(inp)\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix1], trainable=False)(inp) # trainable=True!\n",
    "    x = Dropout(0.1)(x)\n",
    "    \n",
    "    #LSTM\n",
    "    attention_input = (CuDNNLSTM(128, return_sequences=True))(x)\n",
    "        \n",
    "    #LINE1: maxpool \n",
    "    max_pool = Dropout(0.2)(attention_input)\n",
    "    max_pool = MaxPooling1D()(max_pool) # size 25 when concatenating\n",
    "    #LINE2: avgpool\n",
    "    avg_pool = Dropout(0.2)(attention_input)\n",
    "    avg_pool = AveragePooling1D()(avg_pool)\n",
    "    \n",
    "    #LINE3: attention\n",
    "    print(attention_input.shape) # input 256,\n",
    "    attention_probs = Dense(units=128, activation='softmax', name='attention_probs')(attention_input)  \n",
    "    print(attention_probs.shape)\n",
    "    attention_mul = Multiply()([attention_input, attention_probs])\n",
    "    print(f'Attention_mul shape: {attention_mul.get_shape()}') # (?, 50, 256)\n",
    "    print(max_pool) # (?, 25, 256)\n",
    "    \n",
    "    \n",
    "    # EMBEDDING 2\n",
    "    x2 = Embedding(max_features, embed_size, weights=[embedding_matrix2], trainable=False)(inp) # trainable=True!\n",
    "    x2 = Dropout(0.1)(x2)\n",
    "    \n",
    "    #LSTM\n",
    "    attention_input2 = CuDNNLSTM(128, return_sequences=True)(x2)\n",
    "    #LINE1: maxpool \n",
    "    max_pool2 = Dropout(0.2)(attention_input2)\n",
    "    max_pool2 = MaxPooling1D()(max_pool2) # size 25 when concatenating\n",
    "    #LINE2: avgpool\n",
    "    avg_pool2 = Dropout(0.2)(attention_input2)\n",
    "    avg_pool2 = AveragePooling1D()(avg_pool2)\n",
    "    #LINE3: attention\n",
    "    attention_probs2 = Dense(units=128, activation='softmax', name='attention_probs2')(attention_input2)  \n",
    "    attention_mul2 = Multiply()([attention_input2, attention_probs2])\n",
    "\n",
    "    # CONCAT\n",
    "    #conc = Concatenate(axis=1)([attention_mul, max_pool, avg_pool, conv1d])\n",
    "    conc = Concatenate(axis=1)([attention_mul, max_pool, avg_pool, attention_mul2, max_pool2, avg_pool2])\n",
    "    \n",
    "    print(f'Concat shape: {conc.get_shape()}')\n",
    "    \n",
    "    #conc = Attention(512, 50)(conc)\n",
    "    conc = Attention(200+0)(conc) # 50 from attention (2-way lstm?), 25 from maxpooling1D, 25 from avg_pool, 11 CNN maxpool\n",
    "    \n",
    "    # linear, relu, bnorm, dropout, out\n",
    "#conc = Dense(50, activation=\"relu\")(conc)\n",
    "    conc = Dense(100, activation=\"relu\")(conc)  # no change in running time if 50 or 10   \n",
    "    \n",
    "    #conc = Activation('relu')(conc)\n",
    "    \n",
    "    #conc = BatchNormalization()(conc) #effec? cost on speed?\n",
    "    conc = Dropout(0.1)(conc)\n",
    "    x = Dense(1, activation=\"sigmoid\")(conc)\n",
    "    return Model(inputs=inp, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BI-directional VERSION\n",
    "\n",
    "def create_model2x128_bi():\n",
    "    embedding_matrix1 = embedding_matrix\n",
    "    embedding_matrix2 = embedding_matrix_fast\n",
    "    #embedding_matrix3 = embedding_matrix_para    \n",
    "    \n",
    "    seed_everything()\n",
    "    inp = Input(shape=(maxlen,))\n",
    "    # x = Embedding(max_features, embed_size, weights=[embedding_matrix1], trainable=True)(inp)\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix1], trainable=False)(inp) # trainable=True!\n",
    "    x = Dropout(0.1)(x)\n",
    "    \n",
    "    #LSTM\n",
    "    attention_input = Bidirectional(CuDNNLSTM(128, return_sequences=True))(x)\n",
    "    #attention_input = CuDNNLSTM(128, return_sequences=True, \n",
    "    #                            tf.keras.initializers.he_normal(seed=SEED))(x) # keyword follows...\n",
    "        \n",
    "    #LINE1: maxpool \n",
    "    max_pool = Dropout(0.2)(attention_input)\n",
    "    max_pool = MaxPooling1D()(max_pool) # size 25 when concatenating\n",
    "    #LINE2: avgpool\n",
    "    avg_pool = Dropout(0.2)(attention_input)\n",
    "    avg_pool = AveragePooling1D()(avg_pool)\n",
    "    #LINE3: attention\n",
    "    \n",
    "    print(attention_input.shape) # input 256,\n",
    "\n",
    "    attention_probs = Dense(units=256, activation='softmax', name='attention_probs')(attention_input)  \n",
    "    print(attention_probs.shape)\n",
    "    \n",
    "    attention_mul = Multiply()([attention_input, attention_probs])\n",
    "    print(f'Attention_mul shape: {attention_mul.get_shape()}') # (?, 50, 256)\n",
    "    print(max_pool) # (?, 25, 256)\n",
    "    \n",
    "    \n",
    "    # EMBEDDING 2\n",
    "    x2 = Embedding(max_features, embed_size, weights=[embedding_matrix2], trainable=False)(inp) # trainable=True!\n",
    "    x2 = Dropout(0.1)(x2)\n",
    "    \n",
    "    #LSTM\n",
    "    attention_input2 = Bidirectional(CuDNNLSTM(128, return_sequences=True))(x2)\n",
    "    #LINE1: maxpool \n",
    "    max_pool2 = Dropout(0.2)(attention_input2)\n",
    "    max_pool2 = MaxPooling1D()(max_pool2) # size 25 when concatenating\n",
    "    #LINE2: avgpool\n",
    "    avg_pool2 = Dropout(0.2)(attention_input2)\n",
    "    avg_pool2 = AveragePooling1D()(avg_pool2)\n",
    "    #LINE3: attention\n",
    "    attention_probs2 = Dense(units=256, activation='softmax', name='attention_probs2')(attention_input2)  \n",
    "    attention_mul2 = Multiply()([attention_input2, attention_probs2])\n",
    "\n",
    "    # CONCAT\n",
    "    #conc = Concatenate(axis=1)([attention_mul, max_pool, avg_pool, conv1d])\n",
    "    conc = Concatenate(axis=1)([attention_mul, max_pool, avg_pool, attention_mul2, max_pool2, avg_pool2])\n",
    "    \n",
    "    print(f'Concat shape: {conc.get_shape()}')\n",
    "    \n",
    "    #conc = Attention(512, 50)(conc)\n",
    "    conc = Attention(200+0)(conc) # 50 from attention (2-way lstm?), 25 from maxpooling1D, 25 from avg_pool, 11 CNN maxpool\n",
    "    \n",
    "    # linear, relu, bnorm, dropout, out\n",
    "#conc = Dense(50, activation=\"relu\")(conc)\n",
    "    conc = Dense(100, activation=\"relu\")(conc)  # no change in running time if 50 or 10   \n",
    "    \n",
    "    #conc = Activation('relu')(conc)\n",
    "    \n",
    "    #conc = BatchNormalization()(conc) #effec? cost on speed?\n",
    "    conc = Dropout(0.1)(conc)\n",
    "    x = Dense(1, activation=\"sigmoid\")(conc)\n",
    "    return Model(inputs=inp, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#15 - 3 embedding  3x64 LSTM           8 min epoch  0.687\n",
    "# 64 lstm     Double embedding lines\n",
    "# 4 epoch?\n",
    "\n",
    "# 128 to 84 sec with LSTM 128 -> 64: about 1/3 of time away\n",
    "\n",
    "#2 rounds. Average Test-F1: 0.68723\n",
    "#STD: 0.00101, low to high: 0.00202\n",
    "#Time: 14.991060626506805 min\n",
    "\n",
    "\n",
    "def create_model3x64():\n",
    "    embedding_matrix1 = embedding_matrix\n",
    "    embedding_matrix2 = embedding_matrix_fast\n",
    "    embedding_matrix3 = embedding_matrix_para\n",
    "    \n",
    "    seed_everything()\n",
    "    inp = Input(shape=(maxlen,))\n",
    "    # x = Embedding(max_features, embed_size, weights=[embedding_matrix1], trainable=True)(inp)\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix1], trainable=False)(inp) # trainable=True!\n",
    "    x = Dropout(0.1)(x)\n",
    "    \n",
    "    \n",
    "    # EMBEDDING 1\n",
    "    #LSTM\n",
    "    attention_input = (CuDNNLSTM(64, return_sequences=True))(x)\n",
    "    #attention_input = CuDNNLSTM(128, return_sequences=True, \n",
    "    #                            tf.keras.initializers.he_normal(seed=SEED))(x) # keyword follows...        \n",
    "    #LINE1: maxpool \n",
    "    max_pool = Dropout(0.2)(attention_input)\n",
    "    max_pool = MaxPooling1D()(max_pool) # size 25 when concatenating\n",
    "    #LINE2: avgpool\n",
    "    avg_pool = Dropout(0.2)(attention_input)\n",
    "    avg_pool = AveragePooling1D()(avg_pool)\n",
    "    #LINE3: attention\n",
    "    print(attention_input.shape) # input 256,\n",
    "    # attn 64 works for both lstm 128 and 64?\n",
    "    attention_probs = Dense(units=64, activation='softmax', name='attention_probs')(attention_input)  \n",
    "    print(attention_probs.shape)\n",
    "    attention_mul = Multiply()([attention_input, attention_probs])\n",
    "    print(f'Attention_mul shape: {attention_mul.get_shape()}') # (?, 50, 256)\n",
    "    print(max_pool) # (?, 25, 256)\n",
    "    \n",
    "    \n",
    "    # EMBEDDING 2\n",
    "    x2 = Embedding(max_features, embed_size, weights=[embedding_matrix2], trainable=False)(inp) # trainable=True!\n",
    "    x2 = Dropout(0.1)(x2)\n",
    "    #LSTM\n",
    "    attention_input2 = CuDNNLSTM(64, return_sequences=True)(x2)\n",
    "    #LINE1: maxpool \n",
    "    max_pool2 = Dropout(0.2)(attention_input2)\n",
    "    max_pool2 = MaxPooling1D()(max_pool2) # \n",
    "    #LINE2: avgpool\n",
    "    avg_pool2 = Dropout(0.2)(attention_input2)\n",
    "    avg_pool2 = AveragePooling1D()(avg_pool2)\n",
    "    #LINE3: attention\n",
    "    attention_probs2 = Dense(units=64, activation='softmax', name='attention_probs2')(attention_input2)  \n",
    "    attention_mul2 = Multiply()([attention_input2, attention_probs2])\n",
    "\n",
    "    # EMBEDDING 3\n",
    "    x3 = Embedding(max_features, embed_size, weights=[embedding_matrix3], trainable=False)(inp) # trainable=True!\n",
    "    x3 = Dropout(0.1)(x3)\n",
    "    #LSTM\n",
    "    attention_input3 = CuDNNLSTM(64, return_sequences=True)(x3)\n",
    "    #LINE1: maxpool \n",
    "    max_pool3 = Dropout(0.2)(attention_input3)\n",
    "    max_pool3 = MaxPooling1D()(max_pool3) \n",
    "    #LINE2: avgpool\n",
    "    avg_pool3 = Dropout(0.2)(attention_input3)\n",
    "    avg_pool3 = AveragePooling1D()(avg_pool3)\n",
    "    #LINE3: attention\n",
    "    attention_probs3 = Dense(units=64, activation='softmax', name='attention_probs3')(attention_input3)  \n",
    "    attention_mul3 = Multiply()([attention_input3, attention_probs3])\n",
    "    \n",
    "\n",
    "    #LINE4 CONVOLUTION\n",
    "    #conv1d = Conv1D(256, 7, activation='relu')(x)\n",
    "            #conv1d = Conv1D(256, 3, activation='relu')(conv1d)\n",
    "    #conv1d = MaxPooling1D(pool_size=8)(conv1d)\n",
    "            #conv1d = GlobalMaxPool1D()(conv1d)\n",
    "            #conv1d = Dropout(0.2)(conv1d)\n",
    "    #print(conv1d)\n",
    "    \n",
    "    # CONCAT\n",
    "    #conc = Concatenate(axis=1)([attention_mul, max_pool, avg_pool, conv1d])\n",
    "    #conc = Concatenate(axis=1)([attention_mul, max_pool, avg_pool, attention_mul2, max_pool2, avg_pool2])\n",
    "    conc = Concatenate(axis=1)([attention_mul, max_pool, avg_pool, attention_mul2, max_pool2, avg_pool2, attention_mul3, max_pool3, avg_pool3])\n",
    "    \n",
    "    print(f'Concat shape: {conc.get_shape()}')\n",
    "    \n",
    "    #conc = Attention(512, 50)(conc)  \n",
    "    conc = Attention(300+0)(conc) # 50 from attention (2-way lstm?), 25 from maxpooling1D, 25 from avg_pool, 11 CNN maxpool\n",
    "                                    # 100 per emb line\n",
    "    \n",
    "    # linear, relu, bnorm, dropout, out\n",
    "#conc = Dense(50, activation=\"relu\")(conc)\n",
    "    conc = Dense(100, activation=\"relu\")(conc)  # no change in running time if 50 or 10.\n",
    "    \n",
    "    \n",
    "    #conc = Activation('relu')(conc)\n",
    "    \n",
    "    #conc = BatchNormalization()(conc) #effec? cost on speed?\n",
    "    conc = Dropout(0.1)(conc)\n",
    "    x = Dense(1, activation=\"sigmoid\")(conc)\n",
    "    return Model(inputs=inp, outputs=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra split for testing only\n",
    "data are\n",
    "train_x, train_y\n",
    "test_x\n",
    "\n",
    "In Final test_x is read from file. \n",
    "Here for testing we use part of labeled data to test_x"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Train / Val / Test -split\n",
    "seed_everything()\n",
    "X_tra, X_val, X_test, y_tra, y_val, y_test, permutation = train_validate_test_split(train_x, train_y, \n",
    "                                train_percent=0.90, validate_percent=0, random_state=SEED+2)\n",
    "\n",
    "print(len(X_tra))\n",
    "print(len(X_val))\n",
    "print(len(X_test))\n",
    "\n",
    "train_x = X_tra\n",
    "train_y = y_tra\n",
    "test_x = X_test\n",
    "test_y = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudo-labeling\n",
    "\n",
    "2x64LSTM with glove+fast is 8 minute? 0.687 ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train fast Pseudo-classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Train classifier using 100% training data.\n",
    "\n",
    "2) Use it to predict the test-data - test_x\n",
    "\n",
    "3) Pick highest confident predictions (close to 0 or close to 1) and assign the predicted label: as pseudo-label\n",
    "\n",
    "4) add by combining to training-set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores = []\n",
    "all_train_preds = [] # original data\n",
    "all_train_preds_plus = [] # for original + pseudo-labeled\n",
    "all_final_preds = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 50, 64)\n",
      "(?, 50, 64)\n",
      "Attention_mul shape: (?, 50, 64)\n",
      "Tensor(\"max_pooling1d_3/Squeeze:0\", shape=(?, 25, 64), dtype=float32)\n",
      "Concat shape: (?, 200, 64)\n",
      "Train on 1175509 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      " - 75s - loss: 9.8236\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "208cbc77a6484578b97578b55e471e67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=41), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "best threshold is 0.3800 with F1 score: 0.6479\n",
      "Test F1 0.63977 (130613 samples)\n",
      "\n",
      "(?, 50, 64)\n",
      "(?, 50, 64)\n",
      "Attention_mul shape: (?, 50, 64)\n",
      "Tensor(\"max_pooling1d_5/Squeeze:0\", shape=(?, 25, 64), dtype=float32)\n",
      "Concat shape: (?, 200, 64)\n",
      "Train on 1175509 samples, validate on 0 samples\n",
      "Epoch 1/1\n",
      " - 75s - loss: 9.8454\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c6e7101fecb448ebd3517d235060771",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=41), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "best threshold is 0.3500 with F1 score: 0.6470\n",
      "Test F1 0.64886 (130613 samples)\n",
      "\n",
      "2 rounds. Average Test-F1: 0.64431\n",
      "STD: 0.00455, low to high: 0.00909\n",
      "Time: 3.766754953066508 min\n"
     ]
    }
   ],
   "source": [
    "# About <8 minute, 0.68+  '2X64' LSTM 2-embedding model\n",
    "\n",
    "#test\n",
    "#n_epochs=1  \n",
    "#folds = 2\n",
    "\n",
    "n_epochs=7  \n",
    "folds = 2\n",
    "model_scores = []\n",
    "\n",
    "training_start=time.time()\n",
    "for i in range(folds):\n",
    "\n",
    "    # Train / Val / Test -split\n",
    "    seed_everything()\n",
    "    X_tra, X_val, X_test, y_tra, y_val, y_test, permutation = train_validate_test_split(train_x, train_y, \n",
    "                                    train_percent=TRAIN_PERCENT, validate_percent=0, random_state=SEED+30+i+2)\n",
    "    \n",
    "    #model = Model(inputs=inp, outputs=x)\n",
    "    model= create_model2x64()\n",
    "    #model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy',f1])\n",
    "   \n",
    "    #model.compile(loss=[focal_loss(alpha=.25, gamma=2)], optimizer='rmsprop', metrics=['accuracy',f1])\n",
    "    model.compile(loss=[focal_loss(alpha=.25, gamma=2)], optimizer='rmsprop', metrics=None)\n",
    "\n",
    "    seed_everything()\n",
    "    \n",
    "    if RUN_LOCAL: # progress bar and predictions on dev set\n",
    "        model.fit(X_tra, y_tra, batch_size=batch_size, epochs=n_epochs, validation_data=(X_test, y_test)); #verbose=2\n",
    "        \n",
    "    else:\n",
    "        model.fit(X_tra, y_tra, batch_size=batch_size, epochs=n_epochs, validation_data=(X_val, y_val), verbose=2); #verbose=2\n",
    "\n",
    "        # train_x vs X_tra  real train-set or this parts 0.9 of it\n",
    "    train_preds = model.predict(train_x, batch_size=1024)\n",
    "    delta = bestThresshold(train_y,train_preds)\n",
    "\n",
    "    test_preds = model.predict(X_test, batch_size=1024)\n",
    "    #print(f'Len test preds: {len(test_preds)}')    \n",
    "    score = f1_score(y_test, np.array(test_preds)>delta)\n",
    "    print(f'Test F1 {score:.5f} ({len(test_preds)} samples)')\n",
    "    print()\n",
    "    model_scores.append(score)    \n",
    "    # PSEUDO\n",
    "    all_train_preds.append( train_preds ) # for counting common F1-delta\n",
    "\n",
    "    all_final_preds.append(  model.predict(test_x, batch_size=1024)  )\n",
    "    \n",
    "    \n",
    "duration = time.time() - training_start\n",
    "print(f'{len(model_scores)} rounds. Average Test-F1: {np.mean(model_scores):.5f}') \n",
    "print(f'STD: {np.std(model_scores):.5f}, low to high: {np.max(model_scores) - np.min(model_scores):.5f}')    \n",
    "print(f'Time: {duration/60} min')\n",
    "\n",
    "# all_scores.append([np.mean(model_scores), model_scores ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(LOGFILE, \"a\") as myfile:\n",
    "    myfile.write(f'Pseudo-training done. Time passed: {(time.time()-start)/60:.2f} min\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Final-predictions to mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "new1 = np.hstack( all_final_preds)\n",
    "new2 = pd.DataFrame(new1)\n",
    "test_preds = np.mean(new2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Pseudo-labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pse = pd.DataFrame(test_preds)\n",
    "df_pse.columns = ['preds']\n",
    "#df_pse['y'] = y_test"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_pse = pd.DataFrame(test_preds)\n",
    "df_pse.columns = ['preds']\n",
    "df_pse['y'] = y_test # we dont have y in final run (test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pse['ones'] = df_pse['preds'].apply(lambda x: x>= 0.58)  #0.57\n",
    "df_pse['zeros'] = df_pse['preds'].apply(lambda x: x<= 0.08)  # 0.09 ->  0.01 smaller\n",
    "df_pse['ones'] =  (df_pse['ones'] == True).astype(int)\n",
    "df_pse['zeros'] = (df_pse['zeros'] == True).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(df_pse['ones']==1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " len 130610\n",
    " \n",
    " pred >= 0.9  0 items\n",
    " pred >= 0.8  13 items   all correct, true y 1\n",
    "     >= 0.7   124    correct 120\n",
    "     >= 0.6   654    correct 591\n",
    "     \n",
    "     >=0.5    2703   2291\n",
    "     \n",
    "Zeros  <= 0.1    97955 correct 97733\n",
    "       <= 0.115  108975 correct 108485\n",
    "     "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# ACCURACY - ONES - compare those pseudo labeled True \n",
    "sample = df_pse[df_pse['ones']==1]\n",
    "\n",
    "size = len(sample)\n",
    "print(f'Sample: {size}')\n",
    "\n",
    "correct = sum(sample['y']==sample['ones'])\n",
    "print(f'Correct: {correct}')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# ACCURACY - ZEROS - compare those pseudo labeled True \n",
    "sample_zero = df_pse[df_pse['zeros']==1]\n",
    "\n",
    "size_zero = len(sample_zero)\n",
    "print(f'Sample: {size_zero}')\n",
    "\n",
    "correct_zero = sum(sample_zero['y']!= sample_zero['zeros'])   # !=  , ie true is 1\n",
    "print(f'Correct: {correct_zero}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build pseudo training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexes of matching rows\n",
    "idx = df_pse.index[(df_pse['ones']==1 ) | (df_pse['zeros']==1)].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pseudo_x = X_test[idx]\n",
    "train_pseudo_y = y_test[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28599"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_pseudo_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine real train + pseudo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONCAT real train + pseudo\n",
    "\n",
    "train_plus_x = np.concatenate((train_x, train_pseudo_x), axis=0)\n",
    "train_plus_y = np.concatenate((train_y, train_pseudo_y), axis=0)\n",
    " \n",
    "# random permutation\n",
    "rand = np.random.permutation(len(train_plus_y))\n",
    "train_plus_x = train_plus_x[rand]\n",
    "train_plus_y = train_plus_y[rand]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Stratified K Fold to improve results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([ 266945,  266946,  266947, ..., 1334718, 1334719, 1334720]),\n",
       "  array([     0,      1,      2, ..., 266942, 266943, 266944])),\n",
       " (array([      0,       1,       2, ..., 1334718, 1334719, 1334720]),\n",
       "  array([266945, 266946, 266947, ..., 534039, 534040, 534041])),\n",
       " (array([      0,       1,       2, ..., 1334718, 1334719, 1334720]),\n",
       "  array([531692, 531699, 531742, ..., 800915, 800916, 800917]))]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if make 5 or 6 splits, can use only 2-4 of them also. Each fold leaves different 1/5 or 1/6 out?\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "n_splits=5\n",
    "\n",
    "splits = list(StratifiedKFold(n_splits=n_splits, shuffle=False, random_state=SEED).split(train_plus_x, train_plus_y))\n",
    "splits[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splits[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i, (train_idx, valid_idx) in enumerate(splits):    \n",
    "# dig into the list with nested []\n",
    "# train_idx, valid_idx = splits[:1][0][:]\n",
    "# len(train_idx)\n",
    "# len(valid_idx)\n",
    "#train_plus_x[train_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining from 2 hours: 6261.325834989548 s\n",
      "(?, 50, 64)\n",
      "(?, 50, 64)\n",
      "Attention_mul shape: (?, 50, 64)\n",
      "Tensor(\"max_pooling1d_7/Squeeze:0\", shape=(?, 25, 64), dtype=float32)\n",
      "Concat shape: (?, 300, 64)\n",
      "Epoch 1/1\n",
      " - 100s - loss: 9.7434\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea99b3ddd47e4ecd94267f24e7caee28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=41), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "best threshold is 0.2900 with F1 score: 0.6529\n",
      "Test F1 0.65226 (266945 samples)\n",
      "\n",
      "1 rounds. Average Test-F1: 0.65226\n",
      "STD: 0.00000, low to high: 0.00000\n",
      "Time: 2.5764235536257427 min\n"
     ]
    }
   ],
   "source": [
    "# Main train:  2x128 LSTM,   about 16 min epoch, 0.69+, 20+ min on K80\n",
    "\n",
    "# 3x64 runs 120s/epoch on 1070, 5x2x 2min = 20 min\n",
    "# 3x64 on K80  201 s  \n",
    "\n",
    "#test\n",
    "#n_epochs=1 \n",
    "#folds = 1\n",
    "\n",
    "n_epochs=7 #6\n",
    "folds = 3 \n",
    "\n",
    "#    dur training 74 min, 3 folds x 5 epoch = 15 epoch\n",
    "model_scores = []\n",
    "\n",
    "training_start=time.time()\n",
    "\n",
    "# DO NOT EXCEED NR of K-folds (or then code to go back fold 1)\n",
    "assert folds <= n_splits\n",
    "\n",
    "for i in range(folds): \n",
    "    \n",
    "    # Check time remaingin from timelimit 2 hours - if too little remaining, break\n",
    "    time_remain = 7200 - (time.time()-start)\n",
    "    print(f'Remaining from 2 hours: {time_remain} s')\n",
    "    \n",
    "    \n",
    "                        # 5 epochs * 2,5 min(k80 2x64) = 12,5 min/ 750 s +30% from pseudo_labels = 1000\n",
    "                        # this is 3x64 so longer than 150s\n",
    "    spare_time = 2000  # 7 epochs * 201 s = 1400 s  +30% from pseudo_labels = ... (faster batch now)\n",
    "    if time_remain < spare_time:\n",
    "         break\n",
    "           \n",
    "    \n",
    "    # Train / Val / Test -split   TRAIN_PLUS version with pseudo_labeled!\n",
    "#    seed_everything()\n",
    "#    X_tra, X_val, X_test, y_tra, y_val, y_test, permutation = train_validate_test_split(train_plus_x, train_plus_y, \n",
    "#                                    train_percent=TRAIN_PERCENT, validate_percent=0, random_state=SEED)\n",
    " \n",
    "#for i, (train_idx, valid_idx) in enumerate(splits):    \n",
    "\n",
    "    # ASSIGN THE K-FOLD\n",
    "    train_idx, valid_idx = splits[:1+i][0][:]\n",
    "    X_tra = train_plus_x[train_idx]\n",
    "    y_tra = train_plus_y[train_idx]\n",
    "    X_test =train_plus_x[valid_idx]\n",
    "    y_test =train_plus_y[valid_idx]   \n",
    "    \n",
    "    \n",
    "    # random permutation\n",
    " #   rand = np.random.permutation(len(X_tra_plus))\n",
    "#    X_tra = X_tra_plus[rand]\n",
    "#    y_tra = y_tra_plus[rand]\n",
    "    \n",
    "    \n",
    "    #model= create_model2x128_bi()  # create_model2x128()\n",
    "    model= create_model3x64()\n",
    "    \n",
    "    #model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', f1])\n",
    "    #model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy',f1])\n",
    "    #model.compile(loss=[focal_loss(alpha=.25, gamma=2)], optimizer='rmsprop', metrics=['accuracy',f1])\n",
    "    model.compile(loss=[focal_loss(alpha=.25, gamma=2)], optimizer='rmsprop', metrics=None)\n",
    "    \n",
    "    #sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    #model.compile(loss='binary_crossentropy', optimizer=sgd)\n",
    "    #https://faroit.github.io/keras-docs/1.2.2/optimizers/\n",
    "    \n",
    "    seed_everything()\n",
    "    \n",
    "    if RUN_LOCAL: # progress bar and predictions on dev set\n",
    "        hist = model.fit(X_tra, y_tra, batch_size=batch_size, epochs=n_epochs, validation_data=(X_test, y_test)); #verbose=2\n",
    "        \n",
    "    else:\n",
    "        # FINAL ver\n",
    "        #model.fit(X_tra, y_tra, batch_size=batch_size, epochs=n_epochs, validation_data=(X_val, y_val), verbose=2); #verbose=2\n",
    "        model.fit(X_tra, y_tra, batch_size=batch_size, epochs=n_epochs, validation_data=None, verbose=2); #verbose=2\n",
    "        \n",
    "        # TESTING ver\n",
    "        #model.fit(X_tra, y_tra, batch_size=batch_size, epochs=n_epochs, validation_data=(X_test, y_test), verbose=2); #verbose=2        \n",
    "        \n",
    "    # USE ORIGINAL TRAIN-SET     \n",
    "    #train_preds = model.predict(X_tra, batch_size=1024)\n",
    "    train_preds = model.predict(train_x, batch_size=1024)\n",
    "    all_train_preds.append( train_preds ) # for counting common F1-delta\n",
    "    \n",
    "    delta = bestThresshold(train_y,train_preds)\n",
    "\n",
    "    test_preds = model.predict(X_test, batch_size=1024)\n",
    "    #print(f'Len test preds: {len(test_preds)}')    \n",
    "    score = f1_score(y_test, np.array(test_preds)>delta)\n",
    "    print(f'Test F1 {score:.5f} ({len(test_preds)} samples)')\n",
    "    print()\n",
    "    model_scores.append(score)      \n",
    "\n",
    "    all_final_preds.append(  model.predict(test_x, batch_size=1024)  )\n",
    "    \n",
    "duration = time.time() - training_start\n",
    "print(f'{len(model_scores)} rounds. Average Test-F1: {np.mean(model_scores):.5f}') \n",
    "print(f'STD: {np.std(model_scores):.5f}, low to high: {np.max(model_scores) - np.min(model_scores):.5f}')    \n",
    "print(f'Time: {duration/60} min')\n",
    "\n",
    "#all_scores.append([np.mean(model_scores), model_scores ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del model\n",
    "#gc.collect()\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.add(Dense(3, activation='softmax', activity_regularizer=activity_l2(0.0001)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(LOGFILE, \"a\") as myfile:\n",
    "    myfile.write(f'Finished main training at: {(time.time()-start)/60:.2f} min\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test batch 512 instead of 1024\n",
    "\n",
    "# 512 runs slower per epoch, but seems converge faster. reaches conv at 5 epochs instead of 7 \n",
    "# and is over 0.6915 !!\n",
    "\n",
    "# 512 - slower: 140s vs 128s/1024\n",
    "#BUT overfits, cut from 7 to 5 epochs?\n",
    "\n",
    "# 1024 was \n",
    "#3 rounds. Average Test-F1: 0.69069\n",
    "#STD: 0.00268, low to high: 0.00623\n",
    "#Time: 47.08462586005529 min"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "best threshold is 0.3300 with F1 score: 0.7419\n",
    "Test F1 0.69189 (130613 samples)\n",
    "\n",
    "best threshold is 0.3000 with F1 score: 0.7440\n",
    "Test F1 0.68698 (130613 samples)\n",
    "\n",
    "Attention_mul shape: (?, 50, 128)\n",
    "Tensor(\"max_pooling1d_21/Squeeze:0\", shape=(?, 25, 128), dtype=float32)\n",
    "Concat shape: (?, 200, 128)\n",
    "Train on 1175509 samples, validate on 130613 samples\n",
    "Epoch 1/7\n",
    "1175509/1175509 [==============================] - 130s 110us/step - loss: 13.0976 - acc: 0.9448 - f1: 0.2304 - val_loss: 10.9868 - val_acc: 0.9480 - val_f1: 0.2996\n",
    "Epoch 2/7\n",
    "1175509/1175509 [==============================] - 128s 109us/step - loss: 11.1181 - acc: 0.9498 - f1: 0.3664 - val_loss: 10.5598 - val_acc: 0.9569 - val_f1: 0.5253\n",
    "Epoch 3/7\n",
    "1175509/1175509 [==============================] - 126s 107us/step - loss: 10.5579 - acc: 0.9523 - f1: 0.4213 - val_loss: 10.2187 - val_acc: 0.9530 - val_f1: 0.4281\n",
    "Epoch 4/7\n",
    "1175509/1175509 [==============================] - 128s 109us/step - loss: 10.1754 - acc: 0.9536 - f1: 0.4479 - val_loss: 10.1240 - val_acc: 0.9566 - val_f1: 0.5091\n",
    "Epoch 5/7\n",
    "1175509/1175509 [==============================] - 128s 109us/step - loss: 9.8270 - acc: 0.9549 - f1: 0.4722 - val_loss: 10.0373 - val_acc: 0.9539 - val_f1: 0.4454\n",
    "Epoch 6/7\n",
    "1175509/1175509 [==============================] - 130s 111us/step - loss: 9.5070 - acc: 0.9562 - f1: 0.4980 - val_loss: 10.1067 - val_acc: 0.9546 - val_f1: 0.4588\n",
    "Epoch 7/7\n",
    "1175509/1175509 [==============================] - 129s 110us/step - loss: 9.2181 - acc: 0.9573 - f1: 0.5157 - val_loss: 10.1187 - val_acc: 0.9576 - val_f1: 0.5378\n",
    "\n",
    "100% 41/41 [00:07<00:00, 5.73it/s]\n",
    "\n",
    "best threshold is 0.3700 with F1 score: 0.7429\n",
    "Test F1 0.69320 (130613 samples)\n",
    "\n",
    "3 rounds. Average Test-F1: 0.69069\n",
    "STD: 0.00268, low to high: 0.00623\n",
    "Time: 47.08462586005529 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(LOGFILE, \"a\") as myfile:\n",
    "    myfile.write(f'Training done. Time passed: {(time.time()-start)/60:.2f} min\\n')\n",
    "    \n",
    "with open(LOGFILE, \"a\") as myfile:\n",
    "    myfile.write(f'Duration of training: {duration/60:.2f} min\\n')    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "del history\n",
    "del model\n",
    "gc.collect()\n",
    "\n",
    "from keras import backend as K\n",
    "K.clear_session() # remove old model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine predictions from K-fold models"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_preds = np.mean(all_train_preds)\n",
    "train_preds\n",
    "#0.12347124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine version - mean\n",
    "new1 = np.hstack( all_train_preds)\n",
    "new2 = pd.DataFrame(new1)\n",
    "train_preds = np.mean(new2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For test set predictions   \n",
    "new1 = np.hstack( all_final_preds)\n",
    "new2 = pd.DataFrame(new1)\n",
    "final_preds = np.mean(new2, axis=1)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3b1345dcfdb4496aa5f476b87193bbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=41), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "best threshold is 0.3300 with F1 score: 0.6538\n"
     ]
    }
   ],
   "source": [
    "delta = bestThresshold(train_y,train_preds)\n",
    "#score = f1_score(y_test, np.array(test_preds)>delta)\n",
    "#print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "_uuid": "d17dd7b0a92ec98134bf8996fd210edaadf7bed6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qid,prediction\r\n",
      "00014894849d00ba98a9,0\r\n",
      "000156468431f09b3cae,0\r\n",
      "000227734433360e1aae,0\r\n",
      "0005e06fbe3045bd2a92,0\r\n",
      "00068a0f7f41f50fc399,0\r\n",
      "000a2d30e3ffd70c070d,0\r\n",
      "000b67672ec9622ff761,0\r\n",
      "000b7fb1146d712c1105,0\r\n",
      "000d665a8ddc426a1907,0\r\n"
     ]
    }
   ],
   "source": [
    "# if RUN_LOCAL == False:\n",
    "submission = df_test[['qid']].copy()\n",
    "submission['prediction'] = (final_preds > delta).astype(int)\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "!head submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(LOGFILE, \"a\") as myfile:\n",
    "    myfile.write(f'End of code reached. Total time: {(time.time()-start)/60:.2f} min\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
